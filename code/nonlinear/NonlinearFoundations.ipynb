{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94327615-52de-460f-bbdb-f5a0fe6ab26a",
   "metadata": {},
   "source": [
    "# Foundations of Nonlinear Optimization\n",
    "\n",
    "We describe derivatives and gradients in terms of optimizing nonlinear functions before, subsequently, explaining how gradients are used to train neural networks.  Training neural networks is, indeed, an instance of optimizing a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419a797-5ddf-460f-8b20-78a531fb87b1",
   "metadata": {},
   "source": [
    "# Plotting Functions in 3 Dimensions\n",
    "\n",
    "We are familiar most likely with plotting a 3D graph as a surface as shown in the first cell below.  For demonstrating nonlinear optimization with gradients another view may provide a clearer picture of the mechanism, namely a contour plot or level sets, as shown in the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df360b2-e824-4877-a11b-001cd62a8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Surface\" plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "# Make data.\n",
    "x = np.arange(-5, 5, 0.05)\n",
    "y = np.arange(-5, 5, 0.05)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = 50 - x**2 - y**2\n",
    "\n",
    "# Plot the surface.\n",
    "ax.plot_surface(x, y, z, cmap=cm.Greys, linewidth=2, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.zaxis.set_major_locator(LinearLocator(6))\n",
    "ax.zaxis.set_major_formatter('{x:.0f}')\n",
    "plt.savefig(f'symm_contour.jpg', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6e2d9-92bb-4489-ade2-07f9c46b58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot\n",
    "fig, ax = plt.subplots()\n",
    "cp = ax.contour(x, y, z, 10, linewidths=1, cmap=cm.Greys)\n",
    "ax.clabel(cp, inline=True, fontsize=10)\n",
    "ax.scatter(0, 0, s=4, c='k')\n",
    "fig.set_size_inches(5,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a255c-887e-4788-8cb1-8c48ea1b036d",
   "metadata": {},
   "source": [
    "# Derivatives\n",
    "\n",
    "A derivative is the slope of a function at a particular point.  The derivative of lines are easily understood since lines have a constant slope.  For example, the derivative of \n",
    "\n",
    "$y = f \\left( x \\right) = 2x + 5$ \n",
    "\n",
    "is \n",
    "\n",
    "$y^\\prime = f^\\prime \\left( x \\right) = \\frac{dy}{dx} = 2$ \n",
    "\n",
    "and, more generally, the derivative of \n",
    "\n",
    "$y = mx +b$ \n",
    "\n",
    "is \n",
    "\n",
    "$f^\\prime \\left( x \\right) = m$.  \n",
    "\n",
    "The slopes (derivatives) of nonlinear functions vary.  Consider, for example, this parabola:\n",
    "\n",
    "$f \\left( x \\right) = - \\left( x - 2 \\right)^2 = -x^2 + 4x - 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf874c64-ce16-4b6c-847d-f0c011195497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming both numpy and pyplot already imported\n",
    "\n",
    "# create a few functions to make life simpler\n",
    "def f(x):\n",
    "    return -(x-2)**2\n",
    "\n",
    "def f_prime(x):\n",
    "    return -2*x +4\n",
    "\n",
    "def b_calc(m,x,y):\n",
    "    return y-m*x\n",
    "    \n",
    "x = np.arange(0,4,0.02)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,[f(z) for z in x], c='k')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylim(-4,1)\n",
    "''' Draw tangent lines and slopes '''\n",
    "ax.plot(np.arange(0, 4.01, 0.5), [0 for _ in range(9)], c='k', linestyle='dashed', linewidth=1)\n",
    "ax.text(2.75,0.15,'slope = 0')\n",
    "x = 2-np.sqrt(2)\n",
    "m = f_prime(x) \n",
    "b = b_calc(m,2-np.sqrt(2),-2)\n",
    "ax.plot(np.arange(0,1.251,0.25),[m*x+b for x in np.arange(0,1.251,0.25)], c='k', linestyle='dashed', linewidth=1)\n",
    "ax.text(x+0.05,m*x+b,f'slope = {m: .1f}')\n",
    "x = 2+np.sqrt(2)\n",
    "m = f_prime(x) \n",
    "y = -2\n",
    "b = b_calc(m,x,y)\n",
    "ax.plot(np.arange(2.75,4.01,0.25),[m*x+b for x in np.arange(2.75,4.01,0.25)], c='k', linestyle='dashed', linewidth=1)\n",
    "ax.text(x+0.05,m*x+b,f'slope = {m: .1f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e8433-b74d-4334-a955-bf1e27d9a304",
   "metadata": {},
   "source": [
    "For the types of functions we are considering in this notebook, $ f \\left( x\\right) = a x^b$, where $a,b$ are constant coefficients and $x$ is a variable,   derivatives can be computed by this formula for :\n",
    "\n",
    "$f^\\prime \\left( x \\right) = abx^{b-1}$\n",
    "\n",
    "The derivative of the parabola above is\n",
    "\n",
    "$f^\\prime \\left( x \\right) = -2x +4$\n",
    "\n",
    "and so the derivatives for the points shown above are verified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7295b-952c-43e4-aa41-c039ffc8bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining f_prime (unnecessarily)\n",
    "def f_prime(x):\n",
    "    return -2*x + 4\n",
    "    \n",
    "pt = [2-np.sqrt(2), 2.0, 2+np.sqrt(2)]\n",
    "for p in pt:\n",
    "    # print each point and the slope at that point\n",
    "    print(f'x = {p: 0.2f};  slope = {f_prime(p): 0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722f496-d412-41be-92cc-ccf3ac3ea606",
   "metadata": {},
   "source": [
    "# Gradients\n",
    "\n",
    "Gradients are vectors that point in the direction of greatest increase for the function.  The variables <code>x</code> and <code>y</code> are the inputs and <code>z</code> is the output or function value.  So, we are interested in which direction, in terms of <code>x</code> and <code>y</code>, causes the function value to increase most quickly.\n",
    "\n",
    "The gradient for our function is a direction indicated by a vector where each component direction is the partial derivative of the function, that is, the derivative of each input variable individually, as shown below.\n",
    "\n",
    "$f \\left( x , y \\right) = 50 - x^2 - y^2$\n",
    "\n",
    "$\\nabla f \\left( x , y \\right) = \\left[ \\begin{array}{c}\n",
    "                                          \\partial_{x} f \\left( x , y \\right) \\\\  \n",
    "                                          \\partial_{y} f \\left( x , y \\right)  \n",
    "                                        \\end{array}  \n",
    "                                 \\right] = \n",
    "                                 \\left[ \\begin{array}{c}\n",
    "                                          -2x \\\\  \n",
    "                                          -2y  \n",
    "                                        \\end{array}  \n",
    "                                 \\right]$\n",
    "\n",
    "Improving the function value from a current point $x_0$ by taking a step of size $\\alpha$ in the direction of the gradient is done as follows:\n",
    "\n",
    "$\\left( x_1 , y_1 \\right) = \\left( x_0 , y_0 \\right) + \\alpha \\frac{\\nabla f \\left( x , y \\right)}{\\left| \\nabla f \\left( x , y \\right) \\right|_2} $\n",
    "\n",
    "It has been proven that if one takes a sufficiently small step $\\alpha$ then the function value improves, that is:\n",
    "\n",
    "$ f \\left( x_1 , y_1 \\right) > f \\left( x_0 , y_0 \\right)$\n",
    "\n",
    "Optimization by gradient search is, then, taking a number of steps as defined above.  One can use a default step size $\\alpha$ and, if that step size does not result in an increased function value, then the step size can be reduce until the step does result in an increased function value, such as redefining $\\alpha$ using $\\alpha = \\frac{\\alpha}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34815d-3ad5-45a0-85db-ad4c7b3927e7",
   "metadata": {},
   "source": [
    "# Gradient Search\n",
    "\n",
    "We will see in this section how a gradient-based algorithm can find the optimal (maximum) value of the parabola defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad9953-124d-4cf4-9059-0332c9bfe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this code cell self-contained\n",
    "# import packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "# define some functions to use\n",
    "def grad(x,y):\n",
    "    return np.array([-2*x , -2*y])\n",
    "\n",
    "def f(x,y):\n",
    "    return 50 - x**2 - y**2\n",
    "\n",
    "def length(x):\n",
    "    return np.sqrt((x**2).sum())\n",
    "\n",
    "def graph(path, i, dpi):\n",
    "    ''' Make data '''\n",
    "    x = np.arange(-5, 5, 0.05)\n",
    "    y = np.arange(-5, 5, 0.05)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = 50 - x**2 - y**2\n",
    "\n",
    "    ''' create graph '''\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    cp = ax.contour(x, y, z, 10, linewidths=1, cmap=cm.Greys)\n",
    "    ax.clabel(cp, inline=True, fontsize=10)\n",
    "    ax.scatter(0, 0, s=1, c='k')\n",
    "    ax.scatter(*zip(*path), s=3, c='r') #plot gradient search progress\n",
    "    fig.set_size_inches(5,5)\n",
    "    plt.savefig(f'symm_{i}.jpg', dpi=dpi)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "path = []\n",
    "pos = np.array([4,4])\n",
    "path.append(pos)\n",
    "\n",
    "alpha = 0.2\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    g = grad(*pos)\n",
    "    max_alpha = 50\n",
    "    counter = 0\n",
    "    while f(*(pos + alpha * g / length(g))) <= f(*pos) and counter < max_alpha:\n",
    "        counter += 1\n",
    "        alpha = alpha/2\n",
    "    if counter == max_alpha:\n",
    "        done = True\n",
    "    else:\n",
    "        pos = pos + alpha * g / length(g)\n",
    "        path.append(pos)\n",
    "        graph(path, i, 600)\n",
    "        i += 1\n",
    "\n",
    "print(f'Final position: {pos}; f(x,y) = {f(*pos)}')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c005400-59b3-4f90-93a6-b84e37b8ae6b",
   "metadata": {},
   "source": [
    "### Movie of Images\n",
    "\n",
    "[Symmetrical Gradient Example](https://youtu.be/oBCCriH7edQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c02f1-359a-4f40-bcdf-7404527bc01b",
   "metadata": {},
   "source": [
    "## Try Something More Interesting\n",
    "\n",
    "Let's optimize a slightly more interesting function ... one that is not symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a062a1-0183-4274-a5cf-c8e7f1bbbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "# Make data.\n",
    "x = np.arange(-5, 5, 0.05)\n",
    "y = np.arange(-5, 5, 0.05)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = 50 - (x/3)**2 - (y/8)**2\n",
    "\n",
    "# Plot the surface.\n",
    "ax.plot_surface(x, y, z, cmap=cm.Greys, linewidth=2, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.zaxis.set_major_locator(LinearLocator(6))\n",
    "ax.zaxis.set_major_formatter('{x:.0f}')\n",
    "plt.savefig(f'elypt_contour.jpg', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff943c8-9adf-4308-bf89-8d76e2e22b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "def grad(x,y):\n",
    "    return np.array([-2*x/3 , -y/4])\n",
    "\n",
    "def f(x,y):\n",
    "    return 50 - (x/3)**2 - (y/8)**2\n",
    "\n",
    "def length(x):\n",
    "    return np.sqrt((x**2).sum())\n",
    "\n",
    "def graph(path, i, dpi):\n",
    "    ''' Make data '''\n",
    "    x = np.arange(-5, 5, 0.05)\n",
    "    y = np.arange(-5, 5, 0.05)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = 50 - (x/3)**2 - (y/8)**2\n",
    "\n",
    "    ''' create graph '''\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    cp = ax.contour(x, y, z, 10, linewidths=1, cmap=cm.Greys)\n",
    "    ax.clabel(cp, inline=True, fontsize=10)\n",
    "    ax.scatter(0, 0, s=1, c='k')\n",
    "    ax.scatter(*zip(*path), s=3, c='r') #plot gradient search progress\n",
    "    fig.set_size_inches(5,5)\n",
    "    plt.savefig(f'elypt_{i}.jpg', dpi=dpi)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "path = []\n",
    "pos = np.array([4,4])\n",
    "path.append(pos)\n",
    "\n",
    "alpha = 0.2\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    g = grad(*pos)\n",
    "    max_alpha = 50\n",
    "    counter = 0\n",
    "    while f(*(pos + alpha * g / length(g))) <= f(*pos) and counter < max_alpha:\n",
    "        counter += 1\n",
    "        alpha = alpha/2\n",
    "    if counter == max_alpha:\n",
    "        done = True\n",
    "    else:\n",
    "        pos = pos + alpha * g / length(g)\n",
    "        path.append(pos)\n",
    "        graph(path, i, 600)\n",
    "        i += 1\n",
    "\n",
    "print(f'Final position: {pos}; f(x,y) = {f(*pos)}')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a50a2-d523-4bb3-a350-5acf8235337a",
   "metadata": {},
   "source": [
    "### Movie of Images\n",
    "\n",
    "[Elliptical Gradient Example](https://youtu.be/CXo2TWLtw8w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e5808-4766-4881-afca-e62b34e976da",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks take input data and map them into categories, which is a _classification_ process.  In the image below MNIST images composed of 784 numerical values representing grayscale shades are mapped onto 10 categories representing the numerals 0 through 9.  These are the inputs into the neural network and the goal is to train the neural network so that each input of 784 pixels results in a correct classification among the digits at the output side of the neural network.\n",
    "\n",
    "The circles in the diagrams are called neurons or perceptrons and each is represented by a mathematical function.  In the case of such a classification network the last output layer would have one neuron being coded as a 1, indicating the most likely characters, with the remaining neurons have values of 0.  This diagram is a simplification of a neural network that would do a good job of classifying MNIST characters, whereas a more reasonable network would have, for starters, more layers.\n",
    "\n",
    "![neural_net](nn_mnist.jpg)\n",
    "\n",
    "The connections among neurons in sucessive layers indicate where the output values from the first layer of neurons (formulas) serve as inputs to neurons (formulas) in the subsequent layer.\n",
    "\n",
    "We will finish our discussion of gradients applied to neural networks using a PowerPoint presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb0ac3-4406-4891-8e3a-a692936322df",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# More Complicated\n",
    "\n",
    "What if you have a \"mountain range\" and you want to find the best? How will gradient search work?\n",
    "\n",
    "![mountain range](mountain_range.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c336fc-0699-4539-bbf5-3c1a0d8d0205",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2024 - Present: Matthew D. Dean, Ph.D.   \n",
    "Clinical Full Professor of Business Analytics at William \\& Mary.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
